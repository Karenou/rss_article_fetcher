#!/usr/bin/env python3
"""
View Articles Tool
æŸ¥çœ‹å·²æŠ“å–çš„RSSæ–‡ç« 
"""

import sys
import json
from pathlib import Path
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from src.storage import Storage
from src.logger import get_logger


def print_separator(char="=", length=80):
    """æ‰“å°åˆ†éš”çº¿"""
    print(char * length)


def print_article(index, article_data):
    """æ‰“å°å•ç¯‡æ–‡ç« ä¿¡æ¯"""
    print(f"\nğŸ“° æ–‡ç«  #{index}")
    print_separator("-", 80)
    
    # åŸºæœ¬ä¿¡æ¯
    print(f"ğŸ“Œ æ ‡é¢˜: {article_data.get('title', 'N/A')}")
    print(f"ğŸ”— é“¾æ¥: {article_data.get('link', 'N/A')}")
    print(f"ğŸ“… å‘å¸ƒæ—¶é—´: {article_data.get('published', 'N/A')}")
    print(f"ğŸ“° æ¥æº: {article_data.get('source', 'N/A')}")
    
    # æ‘˜è¦
    summary = article_data.get('summary', '')
    if summary:
        print(f"\nğŸ“ è‹±æ–‡æ‘˜è¦:")
        print_separator("-", 80)
        # æ ¼å¼åŒ–æ‘˜è¦ï¼Œæ¯è¡Œæœ€å¤š80å­—ç¬¦
        words = summary.split()
        line = ""
        for word in words:
            if len(line) + len(word) + 1 <= 78:
                line += word + " "
            else:
                print(f"  {line.strip()}")
                line = word + " "
        if line:
            print(f"  {line.strip()}")
    else:
        print(f"\nğŸ“ è‹±æ–‡æ‘˜è¦: (æ— )")
    
    # åŸå§‹æè¿°
    description = article_data.get('description', '')
    if description and description != summary:
        print(f"\nğŸ“„ åŸå§‹æè¿°:")
        print_separator("-", 80)
        # æˆªå–å‰200å­—ç¬¦
        desc_preview = description[:200]
        if len(description) > 200:
            desc_preview += "..."
        print(f"  {desc_preview}")
    
    # å¤„ç†æ—¶é—´
    processed_at = article_data.get('processed_at', '')
    if processed_at:
        print(f"\nâ° å¤„ç†æ—¶é—´: {processed_at}")


def view_all_articles(data_dir: str, limit: int = None, format: str = "text"):
    """æŸ¥çœ‹æ‰€æœ‰æ–‡ç« 
    
    Args:
        data_dir: æ•°æ®ç›®å½•è·¯å¾„
        limit: é™åˆ¶æ˜¾ç¤ºæ•°é‡
        format: è¾“å‡ºæ ¼å¼ (text/json)
    """
    import sqlite3
    
    db_path = Path(data_dir) / "processed_articles.db"
    
    if not db_path.exists():
        print("âŒ æ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨")
        return
    
    try:
        conn = sqlite3.connect(str(db_path))
        cursor = conn.cursor()
        
        # æŸ¥è¯¢æ‰€æœ‰æ–‡ç« 
        cursor.execute('''
            SELECT url, title, source, source_url, description, content, summary, published_at, processed_at
            FROM articles
            ORDER BY processed_at DESC
        ''')
        
        rows = cursor.fetchall()
        conn.close()
        
        if not rows:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ–‡ç« ")
            return
        
        # é™åˆ¶æ•°é‡
        if limit:
            rows = rows[:limit]
        
        total = len(rows)
        
        print_separator("=", 80)
        print(f"  ğŸ“š RSSæ–‡ç« æŸ¥çœ‹å™¨")
        print_separator("=", 80)
        print(f"\næ€»å…±æœ‰ {total} ç¯‡æ–‡ç« ")
        if limit and len(rows) < total:
            print(f"æ˜¾ç¤ºæœ€æ–°çš„ {len(rows)} ç¯‡")
        print()
        
        if format == "json":
            # JSONæ ¼å¼è¾“å‡º
            output = []
            for row in rows:
                output.append({
                    "url": row[0],
                    "title": row[1],
                    "source": row[2],
                    "source_url": row[3],
                    "description": row[4],
                    "content": row[5],
                    "summary": row[6],
                    "published": row[7],
                    "processed_at": row[8]
                })
            print(json.dumps(output, indent=2, ensure_ascii=False))
        else:
            # æ–‡æœ¬æ ¼å¼è¾“å‡º
            for i, row in enumerate(rows, 1):
                article_data = {
                    "url": row[0],
                    "title": row[1],
                    "link": row[0],
                    "source": row[2],
                    "source_url": row[3],
                    "description": row[4],
                    "content": row[5],
                    "summary": row[6],
                    "published": row[7],
                    "processed_at": row[8]
                }
                print_article(i, article_data)
            
            print("\n")
            print_separator("=", 80)
            print(f"  å…±æ˜¾ç¤º {len(rows)} ç¯‡æ–‡ç« ")
            print_separator("=", 80)
    
    except Exception as e:
        print(f"âŒ è¯»å–æ•°æ®åº“å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


def view_latest_articles(data_dir: str, count: int = 5):
    """æŸ¥çœ‹æœ€æ–°çš„Nç¯‡æ–‡ç« """
    view_all_articles(data_dir, limit=count, format="text")


def view_article_by_keyword(data_dir: str, keyword: str):
    """æ ¹æ®å…³é”®è¯æœç´¢æ–‡ç« """
    import sqlite3
    
    db_path = Path(data_dir) / "processed_articles.db"
    
    if not db_path.exists():
        print("âŒ æ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨")
        return
    
    try:
        conn = sqlite3.connect(str(db_path))
        cursor = conn.cursor()
        
        # æœç´¢åŒ¹é…çš„æ–‡ç« 
        keyword_pattern = f"%{keyword}%"
        cursor.execute('''
            SELECT url, title, source, source_url, description, content, summary, published_at, processed_at
            FROM articles
            WHERE title LIKE ? OR source LIKE ? OR summary LIKE ? OR description LIKE ?
            ORDER BY processed_at DESC
        ''', (keyword_pattern, keyword_pattern, keyword_pattern, keyword_pattern))
        
        rows = cursor.fetchall()
        conn.close()
        
        if not rows:
            print(f"âŒ æ²¡æœ‰æ‰¾åˆ°åŒ…å«å…³é”®è¯ '{keyword}' çš„æ–‡ç« ")
            return
        
        print_separator("=", 80)
        print(f"  ğŸ” æœç´¢ç»“æœ: '{keyword}'")
        print_separator("=", 80)
        print(f"\næ‰¾åˆ° {len(rows)} ç¯‡ç›¸å…³æ–‡ç« \n")
        
        for i, row in enumerate(rows, 1):
            article_data = {
                "url": row[0],
                "title": row[1],
                "link": row[0],
                "source": row[2],
                "source_url": row[3],
                "description": row[4],
                "content": row[5],
                "summary": row[6],
                "published": row[7],
                "processed_at": row[8]
            }
            print_article(i, article_data)
        
        print("\n")
        print_separator("=", 80)
        print(f"  å…±æ‰¾åˆ° {len(rows)} ç¯‡æ–‡ç« ")
        print_separator("=", 80)
    
    except Exception as e:
        print(f"âŒ æœç´¢å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


def export_to_json(data_dir: str, output_file: str):
    """å¯¼å‡ºæ–‡ç« åˆ°JSONæ–‡ä»¶"""
    import sqlite3
    
    db_path = Path(data_dir) / "processed_articles.db"
    
    if not db_path.exists():
        print("âŒ æ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨")
        return
    
    try:
        conn = sqlite3.connect(str(db_path))
        cursor = conn.cursor()
        
        cursor.execute('''
            SELECT url, title, source, source_url, description, content, summary, published_at, processed_at
            FROM articles
            ORDER BY processed_at DESC
        ''')
        
        rows = cursor.fetchall()
        conn.close()
        
        if not rows:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ–‡ç« ")
            return
        
        # è½¬æ¢ä¸ºåˆ—è¡¨æ ¼å¼
        output = []
        for row in rows:
            output.append({
                "url": row[0],
                "title": row[1],
                "source": row[2],
                "source_url": row[3],
                "description": row[4],
                "content": row[5],
                "summary": row[6],
                "published": row[7],
                "processed_at": row[8]
            })
        
        # å†™å…¥æ–‡ä»¶
        output_path = Path(output_file)
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(output, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… å·²å¯¼å‡º {len(output)} ç¯‡æ–‡ç« åˆ°: {output_path}")
        print(f"   æ–‡ä»¶å¤§å°: {output_path.stat().st_size} bytes")
    
    except Exception as e:
        print(f"âŒ å¯¼å‡ºå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


def show_statistics(data_dir: str):
    """æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯"""
    import sqlite3
    
    db_path = Path(data_dir) / "processed_articles.db"
    
    if not db_path.exists():
        print("âŒ æ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨")
        return
    
    try:
        conn = sqlite3.connect(str(db_path))
        cursor = conn.cursor()
        
        # æ€»æ–‡ç« æ•°
        cursor.execute("SELECT COUNT(*) FROM articles")
        total = cursor.fetchone()[0]
        
        if total == 0:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ–‡ç« ")
            conn.close()
            return
        
        # æŒ‰æ¥æºç»Ÿè®¡
        cursor.execute('''
            SELECT source, COUNT(*) as count
            FROM articles
            GROUP BY source
            ORDER BY count DESC
        ''')
        sources = cursor.fetchall()
        
        # æŒ‰æ—¥æœŸç»Ÿè®¡
        cursor.execute('''
            SELECT DATE(processed_at) as date, COUNT(*) as count
            FROM articles
            GROUP BY DATE(processed_at)
            ORDER BY date DESC
            LIMIT 10
        ''')
        dates = cursor.fetchall()
        
        conn.close()
        
        print_separator("=", 80)
        print(f"  ğŸ“Š æ–‡ç« ç»Ÿè®¡ä¿¡æ¯")
        print_separator("=", 80)
        
        print(f"\nğŸ“š æ€»æ–‡ç« æ•°: {total}")
        
        print(f"\nğŸ“° æŒ‰æ¥æºç»Ÿè®¡:")
        print_separator("-", 80)
        for source, count in sources:
            percentage = (count / total) * 100
            bar = "â–ˆ" * int(percentage / 2)
            print(f"  {source:30s} {count:3d} ç¯‡ ({percentage:5.1f}%) {bar}")
        
        print(f"\nğŸ“… æŒ‰æ—¥æœŸç»Ÿè®¡ (æœ€è¿‘10å¤©):")
        print_separator("-", 80)
        for date, count in dates:
            percentage = (count / total) * 100
            bar = "â–ˆ" * int(percentage / 2)
            print(f"  {date:12s} {count:3d} ç¯‡ ({percentage:5.1f}%) {bar}")
        
        print()
        print_separator("=", 80)
    
    except Exception as e:
        print(f"âŒ ç»Ÿè®¡å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(
        description="æŸ¥çœ‹å·²æŠ“å–çš„RSSæ–‡ç« ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # æŸ¥çœ‹æœ€æ–°çš„5ç¯‡æ–‡ç« 
  python3 view_articles.py
  
  # æŸ¥çœ‹æœ€æ–°çš„10ç¯‡æ–‡ç« 
  python3 view_articles.py --latest 10
  
  # æŸ¥çœ‹æ‰€æœ‰æ–‡ç« 
  python3 view_articles.py --all
  
  # æœç´¢åŒ…å«å…³é”®è¯çš„æ–‡ç« 
  python3 view_articles.py --search "AI"
  
  # å¯¼å‡ºæ‰€æœ‰æ–‡ç« åˆ°JSON
  python3 view_articles.py --export articles.json
  
  # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
  python3 view_articles.py --stats
  
  # ä»¥JSONæ ¼å¼è¾“å‡º
  python3 view_articles.py --all --format json
        """
    )
    
    parser.add_argument(
        '--data-dir',
        default='/Users/karenou/Desktop/AI/rss_article_fetcher/data',
        help='æ•°æ®ç›®å½•è·¯å¾„'
    )
    
    parser.add_argument(
        '--latest',
        type=int,
        metavar='N',
        help='æŸ¥çœ‹æœ€æ–°çš„Nç¯‡æ–‡ç«  (é»˜è®¤: 5)'
    )
    
    parser.add_argument(
        '--all',
        action='store_true',
        help='æŸ¥çœ‹æ‰€æœ‰æ–‡ç« '
    )
    
    parser.add_argument(
        '--search',
        metavar='KEYWORD',
        help='æœç´¢åŒ…å«å…³é”®è¯çš„æ–‡ç« '
    )
    
    parser.add_argument(
        '--export',
        metavar='FILE',
        help='å¯¼å‡ºæ–‡ç« åˆ°JSONæ–‡ä»¶'
    )
    
    parser.add_argument(
        '--stats',
        action='store_true',
        help='æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯'
    )
    
    parser.add_argument(
        '--format',
        choices=['text', 'json'],
        default='text',
        help='è¾“å‡ºæ ¼å¼ (é»˜è®¤: text)'
    )
    
    args = parser.parse_args()
    
    try:
        if args.stats:
            show_statistics(args.data_dir)
        elif args.export:
            export_to_json(args.data_dir, args.export)
        elif args.search:
            view_article_by_keyword(args.data_dir, args.search)
        elif args.all:
            view_all_articles(args.data_dir, limit=None, format=args.format)
        else:
            # é»˜è®¤æ˜¾ç¤ºæœ€æ–°çš„5ç¯‡
            count = args.latest if args.latest else 5
            view_latest_articles(args.data_dir, count)
    
    except KeyboardInterrupt:
        print("\n\næ“ä½œå·²å–æ¶ˆ")
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
